import { GoogleGenAI, Modality, Type } from "@google/genai";
import type { BgPropImage, VideoPrompts, AiModel } from '../types';

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

export interface PromptSetWithVideo {
    image: { korean: string; english: string };
    video: { korean: string; english: string };
}

export interface PromptSetImageOnly {
    image: { korean: string; english: string };
}

export interface PromptAnalysisResult {
    contextAnalysis: string;
    prompts: Partial<Record<AiModel, PromptSetWithVideo | PromptSetImageOnly>>;
}

export interface StoryFrame {
  frame_number: number;
  prompt_for_image_model: string;
}

export interface StoryPlan {
  frames: StoryFrame[];
}

/**
 * Edits an image using the Gemini 'nano-banana' model.
 * @param base64Image The base64 encoded image string.
 * @param mimeType The MIME type of the image.
 * @param prompt The text prompt for the edit.
 * @returns A promise that resolves to the base64 string of the edited image.
 */
export const editImageWithNanoBanana = async (
  base64Image: string,
  mimeType: string,
  prompt: string
): Promise<string> => {
  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: {
        parts: [
          {
            inlineData: {
              data: base64Image,
              mimeType: mimeType,
            },
          },
          {
            text: prompt,
          },
        ],
      },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error editing image with Gemini:", error);
    throw error;
  }
};

/**
 * Generates a cosplay image using the Gemini 'nano-banana' model.
 * @param mainImageBase64 The base64 encoded image string of the person.
 * @param mainImageMimeType The MIME type of the person's image.
 * @param refImageBase64 The base64 encoded image string of the character.
 * @param refImageMimeType The MIME type of the character's image.
 * @param mainPrompt The main text prompt for the cosplay generation.
 * @param backgroundPrompt The text prompt for the background.
 * @returns A promise that resolves to the base64 string of the generated image.
 */
export const generateCosplayImage = async (
  mainImageBase64: string,
  mainImageMimeType: string,
  refImageBase64: string,
  refImageMimeType: string,
  mainPrompt: string,
  backgroundPrompt: string
): Promise<string> => {
  try {
    const prompt = `${mainPrompt} For the background, use: '${backgroundPrompt || 'a neutral studio background'}'`;

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: {
        parts: [
          {
            inlineData: {
              data: mainImageBase64,
              mimeType: mainImageMimeType,
            },
          },
          { text: 'This is the person whose face must be preserved.' },
          {
            inlineData: {
              data: refImageBase64,
              mimeType: refImageMimeType,
            },
          },
          { text: 'This is the character whose costume and hairstyle should be applied.' },
          {
            text: prompt,
          },
        ],
      },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error generating cosplay image with Gemini:", error);
    throw error;
  }
};


export const generateBackgroundAndPropsImage = async (
  mainImage: {data: string; mime: string},
  backgrounds: ({data: string; mime: string} | null)[],
  props: ({data: string; mime: string} | null)[],
  userPrompt: string
): Promise<string> => {
  try {
    const parts: any[] = [];

    // Main image
    parts.push({ inlineData: { data: mainImage.data, mimeType: mainImage.mime } });
    parts.push({ text: 'This is the main person or subject. Preserve their appearance and place them in a new scene.' });
    
    // Background images
    const validBackgrounds = backgrounds.filter(bg => bg !== null) as {data: string; mime: string}[];
    if (validBackgrounds.length > 0) {
      validBackgrounds.forEach((bg, index) => {
        parts.push({ inlineData: { data: bg.data, mimeType: bg.mime } });
        parts.push({ text: `This is a reference image for the background (Reference ${index + 1}). Use elements from this to create the new background.` });
      });
    }

    // Prop images
    const validProps = props.filter(p => p !== null) as {data: string; mime: string}[];
    if (validProps.length > 0) {
        validProps.forEach((prop, index) => {
            parts.push({ inlineData: { data: prop.data, mimeType: prop.mime } });
            parts.push({ text: `This is a reference image for a prop (Prop ${index + 1}). Incorporate this object into the scene.` });
        });
    }

    // Final user prompt
    const finalPrompt = `
      Instructions:
      1. Take the main person from the first image.
      2. Place them in a new background inspired by the provided background reference images.
      3. Integrate the objects from the prop reference images into the scene naturally.
      4. Follow the user's specific instructions for how to combine everything.
      
      User's instructions: "${userPrompt || 'Create a cohesive and realistic scene combining all the elements.'}"
    `;
    parts.push({ text: finalPrompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: { parts },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error generating background/prop image with Gemini:", error);
    throw error;
  }
};

export const generatePromptForBgProp = async (
  mainImage: {data: string; mime: string},
  backgrounds: ({data: string; mime: string} | null)[],
  props: ({data: string; mime: string} | null)[]
): Promise<string> => {
  try {
    const parts: any[] = [];

    // Main image
    parts.push({ inlineData: { data: mainImage.data, mimeType: mainImage.mime } });
    parts.push({ text: 'This is the main person or subject.' });
    
    // Background images
    const validBackgrounds = backgrounds.filter(bg => bg !== null) as {data: string; mime: string}[];
    if (validBackgrounds.length > 0) {
      validBackgrounds.forEach((bg) => {
        parts.push({ inlineData: { data: bg.data, mimeType: bg.mime } });
      });
       parts.push({ text: 'These are reference images for the background.' });
    }

    // Prop images
    const validProps = props.filter(p => p !== null) as {data: string; mime: string}[];
    if (validProps.length > 0) {
        validProps.forEach((prop) => {
            parts.push({ inlineData: { data: prop.data, mimeType: prop.mime } });
        });
        parts.push({ text: `These are reference images for props.` });
    }

    // Final user prompt
    const finalPrompt = `
      Analyze all the provided images (main subject, backgrounds, and props). Based on the elements in these images, write a creative and descriptive instruction prompt in Korean for an AI image generator. The prompt should explain how to combine the main subject with the backgrounds and props to create a single, cohesive, and interesting new image. The response should be only the Korean prompt, without any other explanatory text.
      For example: "메인 인물을 배경 1의 숲 속에 배치하고, 소품 1의 마법 지팡이를 손에 들게 해주세요. 전체적인 분위기는 배경 2처럼 신비로운 밤으로 만들어주세요."
    `;
    parts.push({ text: finalPrompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: { parts },
    });

    return response.text;

  } catch (error) {
    console.error("Error generating prompt for background/prop tool:", error);
    throw error;
  }
};

export const regeneratePromptVariation = async (
  base64Image: string,
  mimeType: string,
  originalPrompt: string
): Promise<string> => {
  try {
    const systemInstruction = `You are a creative assistant for an AI image generation tool. Your task is to rewrite a user's prompt to provide an interesting variation.
- Analyze the user's uploaded image and their original prompt.
- Generate a single, new prompt that is a creative alternative. It should aim for a similar outcome but explore a different artistic style, mood, composition, or detail.
- The new prompt should be concise and ready to be used directly in an image generation model.
- Your response MUST ONLY be the new prompt text in English, without any introductory phrases, explanations, or quotation marks.`;

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: { 
        parts: [
          { inlineData: { data: base64Image, mimeType: mimeType } },
          { text: `Original prompt: "${originalPrompt}"` }
        ]
      },
      config: {
        systemInstruction,
      },
    });

    return response.text.trim();

  } catch (error) {
    console.error("Error regenerating prompt variation:", error);
    throw error;
  }
};


const promptLangSchema = {
    type: Type.OBJECT,
    properties: {
        korean: { type: Type.STRING },
        english: { type: Type.STRING },
    },
    required: ['korean', 'english'],
};

const promptTypeSchema = {
    type: Type.OBJECT,
    properties: {
        image: promptLangSchema,
        video: promptLangSchema,
    },
    required: ['image', 'video'],
};

const imageOnlyPromptTypeSchema = {
    type: Type.OBJECT,
    properties: {
        image: promptLangSchema,
    },
    required: ['image'],
};

const getSystemInstructionForPrompts = (
    inputType: 'image' | 'text',
    models: AiModel[],
    includeContextAnalysis: boolean,
    templates: Record<AiModel, { image: string; video?: string; }>
) => {
    const analysisTarget = inputType === 'image'
        ? "Analyze the provided image and any additional text."
        : "Analyze the provided text description.";

    const requiredFieldsInRoot = ['prompts'];
    if (includeContextAnalysis) {
        requiredFieldsInRoot.push('contextAnalysis');
    }

    const rootProperties: any = {
        prompts: {
            type: Type.OBJECT,
            properties: {},
            required: models,
        }
    };

    if (includeContextAnalysis) {
        rootProperties.contextAnalysis = { type: Type.STRING, description: 'A brief analysis of the input\'s content, style, and mood in Korean.' };
    }

    const modelPromptSchemas: Record<AiModel, any> = {
        sdxl: imageOnlyPromptTypeSchema,
        gemini: promptTypeSchema,
        midjourney: promptTypeSchema,
        nanoBanana: imageOnlyPromptTypeSchema,
        flux: imageOnlyPromptTypeSchema,
        seedream: imageOnlyPromptTypeSchema,
        qwen: imageOnlyPromptTypeSchema,
        wan2_2: promptTypeSchema,
    };

    for (const model of models) {
        if (modelPromptSchemas[model]) {
            rootProperties.prompts.properties[model] = modelPromptSchemas[model];
        }
    }

    const responseSchema = {
        type: Type.OBJECT,
        properties: rootProperties,
        required: requiredFieldsInRoot,
    };

    const modelMap: Record<AiModel, string> = {
        sdxl: 'SDXL', gemini: 'Gemini', midjourney: 'Midjourney', nanoBanana: 'Nano-Banana', flux: 'FLUX', 
        seedream: 'SEEDREAM 4.0', qwen: 'QWEN', wan2_2: 'WAN2.2'
    };
    const modelListString = models.map(m => modelMap[m] || m).join(', ');
    
    const analysisInstruction = includeContextAnalysis
        ? '1. "contextAnalysis": 입력 내용, 스타일, 분위기에 대한 간략한 한국어 분석.'
        : '';
    const promptInstructionNumber = includeContextAnalysis ? '2.' : '1.';

    const templateInstructions = models.map(model => {
        const modelName = modelMap[model] || model;
        const imageTemplate = templates[model]?.image || 'No image template provided.';
        const videoTemplate = templates[model]?.video;
        let instruction = `${modelName} Image Prompt Template:\n${imageTemplate}`;
        if (videoTemplate) {
            instruction += `\n\n${modelName} Video Prompt Template:\n${videoTemplate}`;
        }
        return instruction;
    }).join('\n\n---\n\n');

    const systemInstruction = `You are an expert prompt engineer for generative AI models. ${analysisTarget}
Based on your analysis, provide the following in a valid JSON object:
${analysisInstruction}
${promptInstructionNumber} "prompts": An object containing prompts for the following AI models: ${modelListString}.
For each model, you MUST follow its specific template instruction provided below to generate the prompts for both image and video (if applicable). For example, if the template asks for keywords, provide keywords. If it asks for a natural sentence, provide a sentence.

--- TEMPLATES ---
${templateInstructions}
--- END TEMPLATES ---

The prompts must be tailored to the specific syntax and strengths of each model as described in their templates. Both Korean and English prompts are required.
For SEEDREAM 4.0, if requested, the image prompt MUST include the phrase "IMG_2094.CR2".
Video prompts should describe motion and change over time.
Nano-Banana prompts should be imperative, edit-style commands.`;

    return { systemInstruction, responseSchema };
};

export const generatePromptsFromImage = async (
    base64Image: string,
    mimeType: string,
    additionalPrompt: string,
    models: AiModel[],
    includeContextAnalysis: boolean,
    templates: Record<AiModel, { image: string; video?: string; }>
): Promise<Partial<PromptAnalysisResult>> => {
    try {
        const { systemInstruction, responseSchema } = getSystemInstructionForPrompts('image', models, includeContextAnalysis, templates);

        const userPrompt = `Analyze the attached image. Additional user context is: "${additionalPrompt || 'None'}"`;

        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: { parts: [
                { inlineData: { data: base64Image, mimeType: mimeType } },
                { text: userPrompt }
            ]},
            config: {
                systemInstruction,
                responseMimeType: "application/json",
                responseSchema
            }
        });
        
        return JSON.parse(response.text);

    } catch (error) {
        console.error("Error generating prompts with Gemini:", error);
        throw error;
    }
};

export const generatePromptsFromText = async (
    prompt: string,
    models: AiModel[],
    includeContextAnalysis: boolean,
    templates: Record<AiModel, { image: string; video?: string; }>
): Promise<Partial<PromptAnalysisResult>> => {
    try {
        const { systemInstruction, responseSchema } = getSystemInstructionForPrompts('text', models, includeContextAnalysis, templates);

        const userPrompt = `Analyze the following description: "${prompt}"`;

        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: userPrompt,
            config: {
                systemInstruction,
                responseMimeType: "application/json",
                responseSchema
            }
        });
        
        return JSON.parse(response.text);

    } catch (error) {
        console.error("Error generating prompts from text with Gemini:", error);
        throw error;
    }
};

export const generateStoryPlan = async (
  userPrompt: string,
  frameCount: number,
  progression: number // 1-10
): Promise<StoryPlan> => {

  const progressionMap: { [key: number]: string } = {
      1: "very subtle changes, almost static",
      2: "very subtle changes",
      3: "subtle changes",
      4: "slight changes",
      5: "moderate changes",
      6: "noticeable changes",
      7: "significant changes",
      8: "major changes",
      9: "very dramatic changes",
      10: "extremely dramatic, almost a different scene"
  };
  const progressionText = progressionMap[progression] || 'moderate changes';

  const systemInstruction = `You are a creative storyboard director for an AI image generator. Your task is to take a user's story prompt and break it down into a sequence of complete, self-contained prompts for an image model.

The image model will only receive the user's original uploaded character image for reference. It will NOT see the previously generated frame. Therefore, each prompt you generate must fully describe the entire scene for that frame.

Your output MUST be a valid JSON object matching the provided schema.

**Instructions:**
1.  **Full Descriptions:** Each prompt must be a complete description of the scene, character's action, and environment. Do not use phrases like "change the pose" or "add a smile." Instead, describe the full state, e.g., "The character is walking through a forest, smiling happily."
2.  **MANDATORY VARIATION:** For every single frame, you **must** create a prompt that specifies a **unique and different pose, expression, background, AND camera angle** from all other frames. The story must show clear and continuous visual progression without repetition.
3.  **Storytelling:** Create a coherent visual narrative that flows logically from one frame to the next.
4.  **Progression Intensity:** Adjust the magnitude of the changes between frames based on the user-provided score (1=subtle, 10=dramatic). A higher score means more significant changes in pose, location, etc., between frames.
5.  **Character Consistency:** The image model is instructed to maintain the character's original appearance (face, hair, clothing) from a reference image. Your prompts should **NOT** describe the character's base appearance (e.g., "a girl with brown hair in a red dress"). Focus only on the dynamic elements: pose, expression, action, background, and camera angle.
6.  **Language:** All prompts for the image model must be in concise, clear English.
7.  **JSON Format:** Adhere strictly to the JSON schema.`;

  const requestPrompt = `
    User Story: "${userPrompt}"
    Number of Frames: ${frameCount}
    Progression Intensity: ${progression} (${progressionText})

    Generate the JSON story plan.
  `;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: requestPrompt,
      config: {
        systemInstruction,
        responseMimeType: 'application/json',
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            frames: {
              type: Type.ARRAY,
              items: {
                type: Type.OBJECT,
                properties: {
                  frame_number: { type: Type.NUMBER, description: "The sequential number of the frame, starting from 1." },
                  prompt_for_image_model: { type: Type.STRING, description: "The English prompt describing the change for this frame." }
                },
                required: ['frame_number', 'prompt_for_image_model']
              }
            }
          },
          required: ['frames']
        }
      }
    });

    const result = JSON.parse(response.text);
    if (!result.frames || !Array.isArray(result.frames)) {
        throw new Error("Invalid story plan format received from API.");
    }
    return result;
  } catch (error) {
    console.error("Error generating story plan:", error);
    throw error;
  }
};

export const generateStoryImageFromSource = async (
  sourceCharacterImage: { data: string; mime: string },
  fullPromptForFrame: string
): Promise<string> => {
  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: {
        parts: [
          {
            inlineData: {
              data: sourceCharacterImage.data,
              mimeType: sourceCharacterImage.mime,
            },
          },
          { text: `Using the provided image as the character reference (it is critical to keep the face, hair, and clothing exactly the same), create a new image with the following scene: "${fullPromptForFrame}"` },
        ],
      },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error generating story image from source with Gemini:", error);
    throw error;
  }
};

export const generateSnapshotImage = async (
  images: { data: string; mimeType: string }[],
  prompt: string
): Promise<string> => {
  try {
    const parts: any[] = [];
    images.forEach((image, index) => {
      parts.push({ inlineData: { data: image.data, mimeType: image.mimeType } });
      parts.push({ text: `This is source person ${index + 1}. Their face and identity MUST be preserved exactly.` });
    });
    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: { parts },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    throw new Error('No image was generated by the model.');
  } catch (error) {
    console.error("Error generating snapshot image with Gemini:", error);
    throw error;
  }
};

export const editImageWithOptionalReference = async (
  mainImage: { data: string; mime: string },
  prompt: string,
  refImage: { data: string; mime: string } | null
): Promise<string> => {
  try {
    const parts: any[] = [
      { inlineData: { data: mainImage.data, mimeType: mainImage.mime } },
    ];

    if (refImage) {
      parts.push({ inlineData: { data: refImage.data, mimeType: refImage.mime } });
    }
    
    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: { parts },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error editing image with reference:", error);
    throw error;
  }
};

export const generateStartEndFrame = async (
  base64Image: string,
  mimeType: string,
  storyPrompt: string,
  direction: 'start-to-end' | 'end-to-start'
): Promise<string> => {
  try {
    const promptText = direction === 'start-to-end'
      ? `This is the starting frame of a scene. Based on the following story, generate the final frame, preserving the character and style of the start frame. Story: "${storyPrompt}"`
      : `This is the final frame of a scene. Based on the following story, generate the starting frame, preserving the character and style of the end frame. Story: "${storyPrompt}"`;

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image-preview',
      contents: {
        parts: [
          { inlineData: { data: base64Image, mimeType: mimeType } },
          { text: promptText },
        ],
      },
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    const candidate = response?.candidates?.[0];
    if (candidate?.content?.parts) {
      for (const part of candidate.content.parts) {
        if (part.inlineData) {
          const editedMimeType = part.inlineData.mimeType;
          const editedBase64 = part.inlineData.data;
          return `data:${editedMimeType};base64,${editedBase64}`;
        }
      }
    }
    
    throw new Error('No image was generated by the model.');

  } catch (error) {
    console.error("Error generating start/end frame with Gemini:", error);
    throw error;
  }
};

export const generateVideoPromptsForFrames = async (
    startImage: { data: string, mime: string },
    endImage: { data: string, mime: string },
    storyPrompt: string
): Promise<VideoPrompts> => {
    const systemInstruction = `You are an expert video prompt engineer. Based on the provided start frame, end frame, and story description, generate concise and effective video generation prompts for the following models: WAN-Diffusion 2.5, Kling 2.5, Midjourney, and VEO 3.0. Your response must be a valid JSON object. The prompts should describe the transition from the start frame to the end frame according to the story. For the VEO 3.0 prompt specifically, it is crucial to provide an extremely detailed and immersive description of all auditory elements. This includes: specific ambient sounds (e.g., 'the gentle rustling of leaves in a light breeze,' 'distant city traffic humming'), distinct sound effects tied to actions (e.g., 'the sharp clink of a glass,' 'a heavy, echoing footstep on stone'), and a description of the musical score (e.g., 'a slow, melancholic piano melody,' 'an upbeat, orchestral score that builds tension'). The sound design should be as descriptive as the visual prompt.`;

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: {
                parts: [
                    { text: 'Start Frame:' },
                    { inlineData: { data: startImage.data, mimeType: startImage.mime } },
                    { text: 'End Frame:' },
                    { inlineData: { data: endImage.data, mimeType: endImage.mime } },
                    { text: `Story: "${storyPrompt}"` }
                ]
            },
            config: {
                systemInstruction,
                responseMimeType: 'application/json',
                responseSchema: {
                    type: Type.OBJECT,
                    properties: {
                        wan2_5: { type: Type.STRING, description: 'Video prompt for WAN-Diffusion 2.5' },
                        kling2_5: { type: Type.STRING, description: 'Video prompt for Kling 2.5' },
                        midjourney: { type: Type.STRING, description: 'Video prompt for Midjourney Video' },
                        veo3_0: { type: Type.STRING, description: 'Video prompt for VEO 3.0, including detailed sound description.' },
                    },
                    required: ['wan2_5', 'kling2_5', 'midjourney', 'veo3_0'],
                }
            }
        });

        return JSON.parse(response.text);

    } catch (error) {
        console.error("Error generating video prompts with Gemini:", error);
        throw error;
    }
};

const MAX_RETRIES = 3;

export const generateProfilePhoto = async (
  baseImages: { data: string; mime: string }[],
  prompt: string
): Promise<string> => {
  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      const imageParts = baseImages.map(image => ({
        inlineData: {
          data: image.data,
          mimeType: image.mime,
        },
      }));

      const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image-preview',
        contents: {
          parts: [
            ...imageParts,
            {
              text: prompt,
            },
          ],
        },
        config: {
          responseModalities: [Modality.IMAGE, Modality.TEXT],
        },
      });

      const imagePart = response.candidates?.[0]?.content?.parts?.find(
        (part) => part.inlineData
      );

      if (imagePart && imagePart.inlineData) {
        const editedMimeType = imagePart.inlineData.mimeType;
        const editedBase64 = imagePart.inlineData.data;
        return `data:${editedMimeType};base64,${editedBase64}`;
      } else {
        throw new Error("No image data found in API response.");
      }
    } catch (error) {
      console.error(`Attempt ${attempt} failed for prompt: "${prompt}"`, error);
      if (attempt === MAX_RETRIES) {
        throw new Error(
          "Failed to generate image after multiple attempts. This could be due to content moderation or API limits."
        );
      }
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
  throw new Error("Image generation failed unexpectedly.");
};

export const modifyPromptForJob = async (originalPrompt: string, jobTitle: string): Promise<string> => {
  try {
    const systemInstruction = `You are a prompt rewriting assistant. Your task is to modify the description of the clothing or outfit in the provided prompt to be appropriate for the job title of a "${jobTitle}".
- You MUST NOT change any other part of the prompt, such as the background, lighting, pose, camera angle, subject's expression, or overall mood.
- Only alter the text describing what the person is wearing.
- If the original prompt does not mention clothing, add a suitable clothing description for the specified job.
- Your response MUST be a JSON object with a single key "modifiedPrompt" which contains the full, rewritten prompt as a string.`;

    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: `Original prompt: "${originalPrompt}"`,
      config: {
        systemInstruction,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            modifiedPrompt: {
              type: Type.STRING,
              description: "The full prompt with only the clothing description modified for the job.",
            },
          },
          required: ["modifiedPrompt"],
        },
      },
    });

    const jsonString = response.text.trim();
    const parsed = JSON.parse(jsonString);

    if (parsed.modifiedPrompt && typeof parsed.modifiedPrompt === 'string') {
      return parsed.modifiedPrompt;
    } else {
      throw new Error("Invalid JSON structure in prompt modification response.");
    }
  } catch (error) {
    console.error(`Failed to modify prompt for job "${jobTitle}":`, error);
    return originalPrompt;
  }
};

export const modifyPromptForConcept = async (originalPrompt: string, concept: string): Promise<string> => {
  try {
    const systemInstruction = `You are a prompt rewriting assistant for an AI image generator. Your task is to incorporate the following concept or object into the user's prompt: "${concept}".
- You MUST integrate the concept/object naturally into the scene. For example, if the concept is "holding a book", add that action. If it's "cyberpunk theme", add descriptive words related to that theme to the background, clothing, or subject.
- You MUST NOT change the core elements of the original prompt, such as the overall composition, camera angle, lighting style (e.g., "cinematic lighting"), subject's pose (unless the concept requires it, like "sitting"), or expression.
- Only add or modify parts of the prompt to include the new concept/object.
- Your response MUST be a JSON object with a single key "modifiedPrompt" which contains the full, rewritten prompt as a string.`;

    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: `Original prompt: "${originalPrompt}"`,
      config: {
        systemInstruction,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            modifiedPrompt: {
              type: Type.STRING,
              description: "The full prompt with the specified concept or object incorporated.",
            },
          },
          required: ["modifiedPrompt"],
        },
      },
    });

    const jsonString = response.text.trim();
    const parsed = JSON.parse(jsonString);

    if (parsed.modifiedPrompt && typeof parsed.modifiedPrompt === 'string') {
      return parsed.modifiedPrompt;
    } else {
      throw new Error("Invalid JSON structure in prompt modification response.");
    }
  } catch (error) {
    console.error(`Failed to modify prompt for concept "${concept}":`, error);
    return originalPrompt;
  }
};